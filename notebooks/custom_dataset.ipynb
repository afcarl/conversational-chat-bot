{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "import random\n",
    "from pprint import pformat\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.metrics import Accuracy, Loss, MetricsLambda, RunningAverage\n",
    "from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n",
    "from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler\n",
    "\n",
    "from pytorch_transformers import AdamW, WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from itertools import chain\n",
    "from pytorch_transformers import GPT2DoubleHeadsModel, GPT2Tokenizer, cached_path, GPT2Config\n",
    "from pytorch_transformers import OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer, OpenAIGPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = GPT2DoubleHeadsModel(GPT2Config()) #.from_pretrained('gpt2')\n",
    "#tokenizer = GPT2Tokenizer(vocab_file='../data/ql_dataset_vocab.json', merges_file='../data/ql_dataset_vocab.json')  #.from_pretrained('gpt2')\n",
    "\n",
    "model = GPT2DoubleHeadsModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50262, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use 5 special tokens:\n",
    "# - <bos> to indicate the start of the sequence\n",
    "# - <eos> to indicate the end of the sequence\n",
    "# - <speaker1> to indicate the beginning and the tokens of an utterance from the user\n",
    "# - <speaker2> to indicate the beginning and the tokens of an utterance from the bot\n",
    "# - <pad> as a padding token to build batches of sequences\n",
    "special_tokens = {\n",
    "    'bos_token': '<bos>',\n",
    "    'eos_token': '<eos>',\n",
    "    'additional_special_tokens': ['<speaker1>', '<speaker2>'],\n",
    "    'pad_token': '<pad>'\n",
    "}\n",
    "\n",
    "# We can add these special tokens to the vocabulary and the embeddings of the model:\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "#model.config.num_special_tokens = len(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Let's define our contexts and special tokens\n",
    "persona = [[\"i\", \"like\", \"playing\", \"football\", \".\"],\n",
    "           [\"i\", \"am\", \"from\", \"NYC\", \".\"]]\n",
    "history = [[\"hello\", \"how\", \"are\", \"you\", \"?\"],\n",
    "           [\"i\", \"am\", \"fine\", \"thanks\", \".\"]]\n",
    "reply = [\"great\", \"to\", \"hear\"]\n",
    "bos, eos, speaker1, speaker2 = \"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\"\n",
    "\n",
    "def build_inputs(persona, history, reply):\n",
    "    # Build our sequence by adding delimiters and concatenating\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + [eos]]\n",
    "    sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s\n",
    "                                for i, s in enumerate(sequence[1:])]\n",
    "    # Build our word, segments and position inputs from the sequence\n",
    "    words = list(chain(*sequence))                          # word tokens\n",
    "    segments = [speaker2 if i % 2 else speaker1             # segment tokens\n",
    "                for i, s in enumerate(sequence) for _ in s]\n",
    "    position = list(range(len(words)))                      # position tokens\n",
    "    return words, segments, position, sequence\n",
    "\n",
    "words, segments, position, sequence = build_inputs(persona, history, reply)\n",
    "\n",
    "# >>> print(sequence)  # Our inputs looks like this:\n",
    "# [['<bos>', 'i', 'like', 'playing', 'football', '.', 'i', 'am', 'from', 'NYC', '.'],\n",
    "#  ['<speaker1>', 'hello', 'how', 'are', 'you', '?'],\n",
    "#  ['<speaker2>', 'i', 'am', 'fine', 'thanks', '.'],\n",
    "#  ['<speaker1>', 'great', 'to', 'hear', '<eos>']]\n",
    "\n",
    "# Tokenize words and segments embeddings:\n",
    "words = tokenizer.convert_tokens_to_ids(words)\n",
    "segments = tokenizer.convert_tokens_to_ids(segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add a distractor to our previously defined persona, history and reply\n",
    "distractor = [\"sorry\", \"to\", \"hear\", \"that\"]\n",
    "\n",
    "# Build & tokenize inputs ending with our distractor like we did with the gold reply\n",
    "words_distractor, segments_distractor, _, _ = build_inputs(persona, history, distractor)\n",
    "words_distractor = tokenizer.convert_tokens_to_ids(words_distractor)\n",
    "segments_distractor = tokenizer.convert_tokens_to_ids(segments_distractor)\n",
    "\n",
    "# Prepare our language modeling targets: keep only the reply segment, -1 on the rest\n",
    "lm_targets = ([-1] * sum(len(s) for s in sequence[:-1])) \\\n",
    "             + [-1] + tokenizer.convert_tokens_to_ids(sequence[-1][1:])\n",
    "lm_distractor = [-1] * len(words_distractor)\n",
    "\n",
    "# Store the position of the last tokens for the next-sentence prediction loss\n",
    "last_token = len(words) - 1\n",
    "last_token_distractor = len(words_distractor) - 1\n",
    "\n",
    "# Now we can pad reply and distractor inputs and targets to the same length\n",
    "padding_length = max(len(words), len(words_distractor))\n",
    "def pad(x, padding):\n",
    "    return x + [padding] * (padding_length - len(x))\n",
    "\n",
    "(words, words_distractor,\n",
    " segments, segments_distractor) = [pad(x, tokenizer.convert_tokens_to_ids('<pad>'))\n",
    "                                   for x in (words, words_distractor,\n",
    "                                             segments, segments_distractor)]\n",
    "\n",
    "(lm_targets, lm_distractor) = [pad(x, -1) for x in (lm_targets, lm_distractor)]\n",
    " \n",
    "# And gather reply and distractor inputs to build the input tensors:\n",
    "# words tokens\n",
    "input_ids = torch.tensor([[words, words_distractor]], dtype=torch.long)\n",
    "# segment tokens\n",
    "token_type_ids = torch.tensor([[segments, segments_distractor]], dtype=torch.long)\n",
    "# Positions tokens can be automatically created by the model as (0, 1, ..., N)\n",
    "# Last tokens location\n",
    "mc_token_ids = torch.tensor([[last_token, last_token_distractor]], dtype=torch.long)\n",
    "# Language modeling labels\n",
    "lm_labels = torch.tensor([[lm_targets, lm_distractor]], dtype=torch.long)\n",
    "# Next-sentence prediction labels\n",
    "mc_labels = torch.tensor([0], dtype=torch.long)  # Gold reply is 1st (index 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(131.6679, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass\n",
    "lm_loss, mc_loss, _, _, _ = model(input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids)\n",
    "\n",
    "# Total loss as a weighted sum\n",
    "lm_coef = 2.0\n",
    "mc_coef = 1.0\n",
    "total_loss = lm_loss * lm_coef + mc_loss * mc_coef\n",
    "total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to train with a custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_dataset = {\n",
    "    'personality': ['My name is John', 'I am a mortgage banker at Quicken Loans',\n",
    "                   'I want to provide you with a mortgage.'],\n",
    "    'utterances': [\n",
    "        {\n",
    "            'candidates': ['hi ! i have three kids . how many do you have ?',\n",
    "                           'awesome ! i own 2 dogs , love them',\n",
    "                           'yes , my favorite is broccoli and tofu in a garlic sauce . yum !',\n",
    "                           'maybe he can skydive to see a better view',\n",
    "                           'Hi there. Can I help you with a mortgage?'],\n",
    "            'history': ['Hi']\n",
    "        },\n",
    "        {\n",
    "            'candidates': ['poetry . roses are red . violet are . . . ?',\n",
    "                          'my father is a member of the army , served for 10 years now .',\n",
    "                          'oh i like mexican food , but my favorite food are cheeseburgers',\n",
    "                          'hey there , are you a mother ?',\n",
    "                          'Fantastic! Can I get your full name?'],\n",
    "            'history': ['Hi', \n",
    "                        'Hi there. Can I help you with a mortgage?',\n",
    "                        'Yes that would be great.']\n",
    "        },\n",
    "        {\n",
    "            'candidates': ['awesome ! i own 2 dogs , love them',\n",
    "                           'yes , my favorite is broccoli and tofu in a garlic sauce . yum !',\n",
    "                           'maybe he can skydive to see a better view',\n",
    "                           'i am good , i just got off work and tired , i have two jobs .'\n",
    "                           'Thank you Zack. Can I get your email address?'],\n",
    "            'history': ['Hi', \n",
    "                        'Hi there. Can I help you with a mortgage?',\n",
    "                        'Yes that would be great.',\n",
    "                        'Fantastic! Can I get your full name?',\n",
    "                        'My name is Zack Jones.']\n",
    "        },\n",
    "        {\n",
    "            'candidates': ['why have you not sent help ? ! the scorpions are stinging my legs ',\n",
    "                           'that is great i am expecting twins in two months . will these be your first kids ?',\n",
    "                           'do you live on a farm or ranch ?',\n",
    "                           'hi how are you doing tonight i am fine .',\n",
    "                           \"i'd love to see her do that .\",\n",
    "                           'That is perfect! I will go ahead and send you an application to get started. Have a great day!'],\n",
    "            'history': ['Hi', \n",
    "                        'Hi there. Can I help you with a mortgage?',\n",
    "                        'Yes that would be great.',\n",
    "                        'Fantastic! Can I get your full name?',\n",
    "                        'My name is Zack Jones.',\n",
    "                        'My email address is zackjones@gmail.com',\n",
    "                        'Thank you, good bye!']\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "ql_dataset = {\n",
    "    'train': [ql_dataset],\n",
    "    'valid': [ql_dataset]\n",
    "}\n",
    "\n",
    "with open('../data/ql_dataset.json', 'w') as file:\n",
    "    json.dump(ql_dataset, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the dataset using our loaded GPT tokenizer\n",
    "def tokenize(obj):\n",
    "    if isinstance(obj, str):\n",
    "        return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "    if isinstance(obj, dict):\n",
    "        return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "    return list(tokenize(o) for o in obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/ql_dataset_vocab.json', 'r') as file:\n",
    "    vocab = json.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]\n",
    "PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_distributed_scalar(scalar, local_rank=-1, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\" Average a scalar over the nodes if we are in distributed training. We use this for distributed evaluation. \"\"\"\n",
    "    if local_rank == -1:\n",
    "        return scalar\n",
    "    scalar_t = torch.tensor(scalar, dtype=torch.float, device=device) / torch.distributed.get_world_size()\n",
    "    torch.distributed.all_reduce(scalar_t, op=torch.distributed.ReduceOp.SUM)\n",
    "    return scalar_t.item()\n",
    "\n",
    "\n",
    "def pad_dataset(dataset, padding=0):\n",
    "    \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padd only batches but this is simpler. \"\"\"\n",
    "    max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
    "    for name in PADDED_INPUTS:\n",
    "        dataset[name] = [x + [padding if name != \"lm_labels\" else -1] * (max_l - len(x)) for x in dataset[name]]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def build_input_from_segments(persona, history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply \"\"\"\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "\n",
    "    instance = {}\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance[\"lm_labels\"] = [-1] * len(instance[\"input_ids\"])\n",
    "    if lm_labels:\n",
    "        instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][1:]\n",
    "    return instance, sequence\n",
    "\n",
    "\n",
    "def get_data_loaders(tokenizer, dataset_path='../data/ql_dataset.json', \n",
    "                     dataset_cache='../data/ql_dataset.json', num_candidates=2, \n",
    "                     personality_permutations=1, max_history=2, distributed=False,\n",
    "                    train_batch_size=2, valid_batch_size=2):\n",
    "    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n",
    "    personachat = get_dataset(tokenizer, dataset_path, dataset_cache)\n",
    "\n",
    "    #logger.info(\"Build inputs and labels\")\n",
    "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
    "    for dataset_name, dataset in personachat.items():\n",
    "        num_candidates_ = len(dataset[0][\"utterances\"][0][\"candidates\"])\n",
    "        if num_candidates_ > 0 and dataset_name == 'train':\n",
    "            num_candidates_ = min(num_candidates, num_candidates_)\n",
    "        for dialog in dataset:\n",
    "            persona = dialog[\"personality\"].copy()\n",
    "            for _ in range(personality_permutations):\n",
    "                for utterance in dialog[\"utterances\"]:\n",
    "                    history = utterance[\"history\"][-(2*max_history+1):]\n",
    "                    for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
    "                        lm_labels = bool(j == num_candidates-1)\n",
    "                        instance, _ = build_input_from_segments(persona, history, candidate, tokenizer, lm_labels)\n",
    "                        for input_name, input_array in instance.items():\n",
    "                            datasets[dataset_name][input_name].append(input_array)\n",
    "                    datasets[dataset_name][\"mc_labels\"].append(num_candidates - 1)\n",
    "                    datasets[dataset_name][\"n_candidates\"] = num_candidates\n",
    "                persona = [persona[-1]] + persona[:-1]  # permuted personalities\n",
    "\n",
    "    #logger.info(\"Pad inputs and convert to Tensor\")\n",
    "    tensor_datasets = {\"train\": [], \"valid\": []}\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
    "        for input_name in MODEL_INPUTS:\n",
    "            tensor = torch.tensor(dataset[input_name])\n",
    "            if input_name != \"mc_labels\":\n",
    "                tensor = tensor.view((-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n",
    "            tensor_datasets[dataset_name].append(tensor)\n",
    "\n",
    "    #logger.info(\"Build train and validation dataloaders\")\n",
    "    train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) if distributed else None\n",
    "    valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset) if distributed else None\n",
    "    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size, shuffle=(not distributed))\n",
    "    valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=valid_batch_size, shuffle=False)\n",
    "\n",
    "    #logger.info(\"Train dataset (Batch, Candidates, Seq length): {}\".format(train_dataset.tensors[0].shape))\n",
    "    #logger.info(\"Valid dataset (Batch, Candidates, Seq length): {}\".format(valid_dataset.tensors[0].shape))\n",
    "    return train_loader, valid_loader, train_sampler, valid_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, dataset_path='../data/ql_dataset.json', \n",
    "                 dataset_cache='../data/ql_dataset.json'):\n",
    "    \"\"\" Get PERSONACHAT from S3 \"\"\"\n",
    "    dataset_path = dataset_path or PERSONACHAT_URL\n",
    "    dataset_cache = dataset_cache + '_' + type(tokenizer).__name__  # Do avoid using GPT cache for GPT-2 and vice-versa\n",
    "    if dataset_cache and os.path.isfile(dataset_cache):\n",
    "        #logger.info(\"Load tokenized dataset from cache at %s\", dataset_cache)\n",
    "        dataset = torch.load(dataset_cache)\n",
    "    else:\n",
    "        #logger.info(\"Download dataset from %s\", dataset_path)\n",
    "        personachat_file = cached_path(dataset_path)\n",
    "        with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            dataset = json.loads(f.read())\n",
    "\n",
    "        #logger.info(\"Tokenize and encode the dataset\")\n",
    "        def tokenize(obj):\n",
    "            if isinstance(obj, str):\n",
    "                return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "            if isinstance(obj, dict):\n",
    "                return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "            return list(tokenize(o) for o in obj)\n",
    "        dataset = tokenize(dataset)\n",
    "        if dataset_cache:\n",
    "            torch.save(dataset, dataset_cache)\n",
    "    return dataset\n",
    "\n",
    "def get_dataset_personalities(tokenizer, dataset_path, dataset_cache=None):\n",
    "    \"\"\" Get personalities from PERSONACHAT \"\"\"\n",
    "    dataset_path = dataset_path or PERSONACHAT_URL\n",
    "    dataset_cache = dataset_cache + '_' + type(tokenizer).__name__  # Do avoid using GPT cache for GPT-2 and vice-versa\n",
    "    if os.path.isfile(dataset_cache):\n",
    "        #logger.info(\"Load tokenized dataset from cache at %s\", dataset_cache)\n",
    "        personachat = torch.load(dataset_cache)\n",
    "    else:\n",
    "        #logger.info(\"Download PERSONACHAT dataset from %s\", dataset_path)\n",
    "        personachat_file = cached_path(dataset_path)\n",
    "        with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            personachat = json.loads(f.read())\n",
    "\n",
    "        #logger.info(\"Tokenize and encode the dataset\")\n",
    "        def tokenize(obj):\n",
    "            if isinstance(obj, str):\n",
    "                return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "            if isinstance(obj, dict):\n",
    "                return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "            return list(tokenize(o) for o in obj)\n",
    "        personachat = tokenize(personachat)\n",
    "        torch.save(personachat, dataset_cache)\n",
    "\n",
    "    #logger.info(\"Filter personalities\")\n",
    "    personalities = []\n",
    "    for dataset in personachat.values():\n",
    "        for dialog in dataset:\n",
    "            personalities.append(dialog[\"personality\"])\n",
    "\n",
    "    #logger.info(\"Gathered {} personalities\".format(len(personalities)))\n",
    "    return personalities\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    distributed=False, local_rank=-1, lr = 6.25e-5, dataset_path='../data/personachat_self_original.json', \n",
    "    dataset_cache=cached_path('../data/personachat_self_original.json'),\n",
    "    model_checkpoint='gpt2', num_candidates=2, max_history=5, train_batch_size=2, valid_batch_size=2,\n",
    "    gradient_accumulation_steps=8, lm_coef=1.0, mc_coef=1.0, max_norm=1.0, n_epochs=10, \n",
    "    personality_permutations=1, eval_before_start=False, device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    fp16=''\n",
    "    ):\n",
    "    '''\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--dataset_path\", type=str, default=\"\", help=\"Path or url of the dataset. If empty download from S3.\")\n",
    "    parser.add_argument(\"--dataset_cache\", type=str, default='./dataset_cache', help=\"Path or url of the dataset cache\")\n",
    "    parser.add_argument(\"--model_checkpoint\", type=str, default=\"openai-gpt\", help=\"Path, url or short name of the model\")\n",
    "    parser.add_argument(\"--num_candidates\", type=int, default=2, help=\"Number of candidates for training\")\n",
    "    parser.add_argument(\"--max_history\", type=int, default=2, help=\"Number of previous exchanges to keep in history\")\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=4, help=\"Batch size for training\")\n",
    "    parser.add_argument(\"--valid_batch_size\", type=int, default=4, help=\"Batch size for validation\")\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=8, help=\"Accumulate gradients on several steps\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=6.25e-5, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--lm_coef\", type=float, default=1.0, help=\"LM loss coefficient\")\n",
    "    parser.add_argument(\"--mc_coef\", type=float, default=1.0, help=\"Multiple-choice loss coefficient\")\n",
    "    parser.add_argument(\"--max_norm\", type=float, default=1.0, help=\"Clipping gradient norm\")\n",
    "    parser.add_argument(\"--n_epochs\", type=int, default=3, help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--personality_permutations\", type=int, default=1, help=\"Number of permutations of personality sentences\")\n",
    "    parser.add_argument(\"--eval_before_start\", action='store_true', help=\"If true start with a first evaluation before training\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"Device (cuda or cpu)\")\n",
    "    parser.add_argument(\"--fp16\", type=str, default=\"\", help=\"Set to O0, O1, O2 or O3 for fp16 training (see apex documentation)\")\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"Local rank for distributed training (-1: not distributed)\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # logging is set to INFO (resp. WARN) for main (resp. auxiliary) process. logger.info => log main process only, logger.warning => log all processes\n",
    "    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "    logger.warning(\"Running process %d\", args.local_rank)  # This is a logger.warning: it will be printed by all distributed processes\n",
    "    logger.info(\"Arguments: %s\", pformat(args))\n",
    "    '''\n",
    "    \n",
    "    args = None\n",
    "    \n",
    "    # Initialize distributed training if needed\n",
    "    distributed = (local_rank != -1)\n",
    "    if distributed:\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        device = torch.device(\"cuda\", local_rank)\n",
    "        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "\n",
    "    #logger.info(\"Prepare tokenizer, pretrained model and optimizer - add special tokens for fine-tuning\")\n",
    "    print(f'{datetime.now()}: Prepare tokenizer, pretrained model and optimizer - add special tokens for fine-tuning')\n",
    "    #tokenizer_class = GPT2Tokenizer if \"gpt2\" in args.model_checkpoint else OpenAIGPTTokenizer\n",
    "    #tokenizer = tokenizer_class.from_pretrained(model_checkpoint)\n",
    "    \n",
    "    #model_class = GPT2DoubleHeadsModel if \"gpt2\" in args.model_checkpoint else OpenAIGPTDoubleHeadsModel\n",
    "    #model = model_class.from_pretrained(args.model_checkpoint)\n",
    "    \n",
    "    #tokenizer.set_special_tokens(SPECIAL_TOKENS)\n",
    "    #model.set_num_special_tokens(len(SPECIAL_TOKENS))\n",
    "    \n",
    "    #model = GPT2DoubleHeadsModel.from_pretrained('gpt2')\n",
    "    #model = GPT2DoubleHeadsModel(GPT2Config())\n",
    "    #tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')\n",
    "    tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "    \n",
    "    # We will use 5 special tokens:\n",
    "    # - <bos> to indicate the start of the sequence\n",
    "    # - <eos> to indicate the end of the sequence\n",
    "    # - <speaker1> to indicate the beginning and the tokens of an utterance from the user\n",
    "    # - <speaker2> to indicate the beginning and the tokens of an utterance from the bot\n",
    "    # - <pad> as a padding token to build batches of sequences\n",
    "    special_tokens = {\n",
    "        'bos_token': '<bos>',\n",
    "        'eos_token': '<eos>',\n",
    "        'additional_special_tokens': ['<speaker1>', '<speaker2>'],\n",
    "        'pad_token': '<pad>'\n",
    "    }\n",
    "\n",
    "    # We can add these special tokens to the vocabulary and the embeddings of the model:\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    #model.config.num_special_tokens = len(special_tokens)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Prepare model for FP16 and distributed training if needed (order is important, distributed should be the last)\n",
    "    if fp16:\n",
    "        from apex import amp  # Apex is only required if we use fp16 training\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=fp16)\n",
    "    if distributed:\n",
    "        model = DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
    "\n",
    "    #logger.info(\"Prepare datasets\")\n",
    "    print(f'{datetime.now()}: prepare datasets')\n",
    "    train_loader, val_loader, train_sampler, valid_sampler = get_data_loaders(tokenizer)\n",
    "\n",
    "    # Training function and trainer\n",
    "    def update(engine, batch):\n",
    "        model.train()\n",
    "        batch = tuple(input_tensor.to(device) for input_tensor in batch)\n",
    "        \n",
    "        lm_loss, mc_loss, _, _ = model(*batch)\n",
    "        loss = (lm_loss * lm_coef + mc_loss * mc_coef) / gradient_accumulation_steps\n",
    "        if fp16:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_norm)\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        if engine.state.iteration % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "    trainer = Engine(update)\n",
    "\n",
    "    # Evaluation function and evaluator (evaluator output is the input of the metrics)\n",
    "    def inference(engine, batch):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch = tuple(input_tensor.to(device) for input_tensor in batch)\n",
    "            input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "            #logger.info(tokenizer.decode(input_ids[0, -1, :].tolist()))\n",
    "            print(f'{datetime.now()}: {tokenizer.decode(input_ids[0, -1, :].tolist())}')\n",
    "            model_outputs = model(input_ids, mc_token_ids, token_type_ids=token_type_ids)\n",
    "            lm_logits, mc_logits = model_outputs[0], model_outputs[1]  # So we can also use GPT2 outputs\n",
    "            lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))\n",
    "            lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)\n",
    "            return (lm_logits_flat_shifted, mc_logits), (lm_labels_flat_shifted, mc_labels)\n",
    "    evaluator = Engine(inference)\n",
    "\n",
    "    # Attach evaluation to trainer: we evaluate when we start the training and at the end of each epoch\n",
    "    trainer.add_event_handler(Events.EPOCH_COMPLETED, lambda _: evaluator.run(val_loader))\n",
    "    if n_epochs < 1:\n",
    "        trainer.add_event_handler(Events.COMPLETED, lambda _: evaluator.run(val_loader))\n",
    "    if eval_before_start:\n",
    "        trainer.add_event_handler(Events.STARTED, lambda _: evaluator.run(val_loader))\n",
    "\n",
    "    # Make sure distributed data samplers split the dataset nicely between the distributed processes\n",
    "    if distributed:\n",
    "        trainer.add_event_handler(Events.EPOCH_STARTED, lambda engine: train_sampler.set_epoch(engine.state.epoch))\n",
    "        evaluator.add_event_handler(Events.EPOCH_STARTED, lambda engine: valid_sampler.set_epoch(engine.state.epoch))\n",
    "\n",
    "    # Linearly decrease the learning rate from lr to zero\n",
    "    scheduler = PiecewiseLinear(optimizer, \"lr\", [(0, lr), (n_epochs * len(train_loader), 0.0)])\n",
    "    trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "\n",
    "    # Prepare metrics - note how we compute distributed metrics \n",
    "    RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n",
    "    metrics = {\"nll\": Loss(torch.nn.CrossEntropyLoss(ignore_index=-1), output_transform=lambda x: (x[0][0], x[1][0])),\n",
    "               \"accuracy\": Accuracy(output_transform=lambda x: (x[0][1], x[1][1]))}\n",
    "    #metrics.update({\"average_nll\": MetricsLambda(average_distributed_scalar, metrics[\"nll\"], args),\n",
    "    #                \"average_accuracy\": MetricsLambda(average_distributed_scalar, metrics[\"accuracy\"], args)})\n",
    "    metrics[\"average_ppl\"] = MetricsLambda(math.exp, metrics[\"nll\"])\n",
    "    for name, metric in metrics.items():\n",
    "        metric.attach(evaluator, name)\n",
    "\n",
    "    # On the main process: add progress bar, tensorboard, checkpoints and save model, configuration and tokenizer before we start to train\n",
    "    if local_rank in [-1, 0]:\n",
    "        pbar = ProgressBar(persist=True)\n",
    "        pbar.attach(trainer, metric_names=[\"loss\"])\n",
    "        evaluator.add_event_handler(Events.COMPLETED, lambda _: pbar.log_message(\"Validation: %s\" % pformat(evaluator.state.metrics)))\n",
    "\n",
    "        tb_logger = TensorboardLogger(log_dir='../logs')\n",
    "        #tb_logger.attach(trainer, log_handler=OutputHandler(tag=\"training\", metric_names=[\"loss\"]), event_name=Events.ITERATION_COMPLETED)\n",
    "        #tb_logger.attach(trainer, log_handler=OptimizerParamsHandler(optimizer), event_name=Events.ITERATION_STARTED)\n",
    "        #tb_logger.attach(evaluator, log_handler=OutputHandler(tag=\"validation\", metric_names=list(metrics.keys()), another_engine=trainer), event_name=Events.EPOCH_COMPLETED)\n",
    "\n",
    "        #checkpoint_handler = ModelCheckpoint(tb_logger.writer.log_dir, 'checkpoint', save_interval=1, n_saved=3)\n",
    "        #trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, \n",
    "        #                          {'mymodel': getattr(model, 'module', model)})  \n",
    "        # \"getattr\" take care of distributed encapsulation\n",
    "\n",
    "        #torch.save(args, tb_logger.writer.log_dir + '/model_training_args.bin')\n",
    "        #getattr(model, 'module', model).config.to_json_file(os.path.join(tb_logger.writer.log_dir, CONFIG_NAME))\n",
    "        #tokenizer.save_vocabulary(tb_logger.writer.log_dir)\n",
    "\n",
    "    # Run the training\n",
    "    trainer.run(train_loader, max_epochs=n_epochs)\n",
    "\n",
    "    # On the main process: close tensorboard logger and rename the last checkpoint (for easy re-loading with OpenAIGPTModel.from_pretrained method)\n",
    "    #if local_rank in [-1, 0] and n_epochs > 0:\n",
    "    #    os.rename(checkpoint_handler._saved[-1][1][-1], os.path.join(tb_logger.writer.log_dir, WEIGHTS_NAME))  # TODO: PR in ignite to have better access to saved file paths (cleaner)\n",
    "    #    tb_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:06.676097: Prepare tokenizer, pretrained model and optimizer - add special tokens for fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:13.530115: prepare datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\altoz\\documents\\projects\\conversational-chat-bot\\env\\lib\\site-packages\\tqdm\\autonotebook\\__init__.py:18: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d58d04361384939b556f2b5b20745ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:14.388106: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:14.438970: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <speaker2>yes that would be great. <speaker1>fantastic! can i get your full name? <speaker2>my name is zack jones. <speaker1>i am good, i just got off work and tired, i have two jobs. thank you zack. can i get your email address? <eos>\n",
      "Validation: {'accuracy': 0.75, 'average_ppl': 27.064928928039876, 'nll': 3.298238754272461}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f237755cbc514597b7e6931cd0feca82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:14.852055: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:14.904399: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <speaker2>yes that would be great. <speaker1>fantastic! can i get your full name? <speaker2>my name is zack jones. <speaker1>i am good, i just got off work and tired, i have two jobs. thank you zack. can i get your email address? <eos>\n",
      "Validation: {'accuracy': 0.75, 'average_ppl': 27.064928928039876, 'nll': 3.298238754272461}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd1416fd83741ae8513b20ac479f9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:15.312525: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:15.359400: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <speaker2>yes that would be great. <speaker1>fantastic! can i get your full name? <speaker2>my name is zack jones. <speaker1>i am good, i just got off work and tired, i have two jobs. thank you zack. can i get your email address? <eos>\n",
      "Validation: {'accuracy': 0.75, 'average_ppl': 27.064928928039876, 'nll': 3.298238754272461}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb106e0f7bbf4e93adff29f24695b3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:15.858977: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:15.911835: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <speaker2>yes that would be great. <speaker1>fantastic! can i get your full name? <speaker2>my name is zack jones. <speaker1>i am good, i just got off work and tired, i have two jobs. thank you zack. can i get your email address? <eos>\n",
      "Validation: {'accuracy': 0.75, 'average_ppl': 22.43836578636115, 'nll': 3.1107722520828247}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de7de84772a4c228853464659fe98d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:16.348688: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:16.407567: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <speaker2>yes that would be great. <speaker1>fantastic! can i get your full name? <speaker2>my name is zack jones. <speaker1>i am good, i just got off work and tired, i have two jobs. thank you zack. can i get your email address? <eos>\n",
      "Validation: {'accuracy': 0.75, 'average_ppl': 22.43836578636115, 'nll': 3.1107722520828247}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f8c78717434fb4a50a201d540f21f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:16.818536: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:16.866419: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <speaker2>yes that would be great. <speaker1>fantastic! can i get your full name? <speaker2>my name is zack jones. <speaker1>i am good, i just got off work and tired, i have two jobs. thank you zack. can i get your email address? <eos>\n",
      "Validation: {'accuracy': 0.75, 'average_ppl': 22.43836578636115, 'nll': 3.1107722520828247}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ec702df5a14c4c87fe31aaf0202dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:17.270847: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:17.326725: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <speaker2>yes that would be great. <speaker1>fantastic! can i get your full name? <speaker2>my name is zack jones. <speaker1>i am good, i just got off work and tired, i have two jobs. thank you zack. can i get your email address? <eos>\n",
      "Validation: {'accuracy': 0.75, 'average_ppl': 22.43836578636115, 'nll': 3.1107722520828247}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee083003ed1d459ba846feb306e70e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:17.800061: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:17.849927: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <speaker2>yes that would be great. <speaker1>fantastic! can i get your full name? <speaker2>my name is zack jones. <speaker1>i am good, i just got off work and tired, i have two jobs. thank you zack. can i get your email address? <eos>\n",
      "Validation: {'accuracy': 0.75, 'average_ppl': 18.235519883892568, 'nll': 2.9033713340759277}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6859beae1446dcab7f85af5181fa94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:18.260432: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:18.311297: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <speaker2>yes that would be great. <speaker1>fantastic! can i get your full name? <speaker2>my name is zack jones. <speaker1>i am good, i just got off work and tired, i have two jobs. thank you zack. can i get your email address? <eos>\n",
      "Validation: {'accuracy': 0.75, 'average_ppl': 18.235519883892568, 'nll': 2.9033713340759277}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a612ebefa02b45f7b2c5bc1383ef3b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:18.730309: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 22:36:18.780200: <bos>my name is john i am a mortgage banker at quicken loans i want to provide you with a mortgage. <speaker2>hi <speaker1>hi there. can i help you with a mortgage? <speaker2>yes that would be great. <speaker1>fantastic! can i get your full name? <speaker2>my name is zack jones. <speaker1>i am good, i just got off work and tired, i have two jobs. thank you zack. can i get your email address? <eos>\n",
      "Validation: {'accuracy': 0.75, 'average_ppl': 18.235519883892568, 'nll': 2.9033713340759277}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(dataset_path='../data/ql_dataset.json', \n",
    "    dataset_cache=cached_path('../data/ql_dataset.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_filtering(logits, top_k=0, top_p=0.0, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
    "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
    "                whose total probability mass is greater than or equal to the threshold top_p.\n",
    "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
    "                the threshold top_p.\n",
    "            threshold: a minimal threshold to keep logits\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # Only work for batch size 1 for now - could update but it would obfuscate a bit the code\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        # Compute cumulative probabilities of sorted tokens\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # Back to unsorted indices and set them to -infinity\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    indices_to_remove = logits < threshold\n",
    "    logits[indices_to_remove] = filter_value\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_sequence(personality, history, tokenizer, model, args, current_output=None):\n",
    "    \n",
    "    #model.to(args['device'])\n",
    "    special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "    if current_output is None:\n",
    "        current_output = []\n",
    "\n",
    "    for i in range(args['max_length']):\n",
    "        instance, sequence = build_input_from_segments(personality, history, current_output, tokenizer, with_eos=False)\n",
    "\n",
    "        input_ids = torch.tensor(instance[\"input_ids\"], device=args['device']).unsqueeze(0)\n",
    "        token_type_ids = torch.tensor(instance[\"token_type_ids\"], device=args['device']).unsqueeze(0)\n",
    "\n",
    "        logits = model(input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "        if \"gpt2\" == args['model']:\n",
    "            logits = logits[0]\n",
    "        logits = logits[0, -1, :] / args['temperature']\n",
    "        logits = top_filtering(logits, top_k=args['top_k'], top_p=args['top_p'])\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        prev = torch.topk(probs, 1)[1] if args['no_sample'] else torch.multinomial(probs, 1)\n",
    "        if i < args['min_length'] and prev.item() in special_tokens_ids:\n",
    "            while prev.item() in special_tokens_ids:\n",
    "                prev = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        if prev.item() in special_tokens_ids:\n",
    "            break\n",
    "        current_output.append(prev.item())\n",
    "\n",
    "    return current_output\n",
    "\n",
    "def get_dataset_personalities(tokenizer, dataset_path, dataset_cache=None):\n",
    "    \"\"\" Get personalities from PERSONACHAT \"\"\"\n",
    "    dataset_path = dataset_path or PERSONACHAT_URL\n",
    "    dataset_cache = dataset_cache + '_' + type(tokenizer).__name__  # Do avoid using GPT cache for GPT-2 and vice-versa\n",
    "    if os.path.isfile(dataset_cache):\n",
    "        #logger.info(\"Load tokenized dataset from cache at %s\", dataset_cache)\n",
    "        personachat = torch.load(dataset_cache)\n",
    "    else:\n",
    "        #logger.info(\"Download PERSONACHAT dataset from %s\", dataset_path)\n",
    "        personachat_file = cached_path(dataset_path)\n",
    "        with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            personachat = json.loads(f.read())\n",
    "\n",
    "        #logger.info(\"Tokenize and encode the dataset\")\n",
    "        def tokenize(obj):\n",
    "            if isinstance(obj, str):\n",
    "                return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "            if isinstance(obj, dict):\n",
    "                return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "            return list(tokenize(o) for o in obj)\n",
    "        personachat = tokenize(personachat)\n",
    "        torch.save(personachat, dataset_cache)\n",
    "\n",
    "    #logger.info(\"Filter personalities\")\n",
    "    personalities = []\n",
    "    for dataset in personachat.values():\n",
    "        for dialog in dataset:\n",
    "            personalities.append(dialog[\"personality\"])\n",
    "\n",
    "    #logger.info(\"Gathered {} personalities\".format(len(personalities)))\n",
    "    return personalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'max_length': 20,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'model': 'gpt2',\n",
    "    'top_k': 0,\n",
    "    'top_p': 0.9,\n",
    "    'no_sample': True,\n",
    "    'min_length': 2,\n",
    "    'temperature': 0.7,\n",
    "    'max_history': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[3666, 1438, 318, 1757],\n",
       "  [40, 716, 257, 13682, 33371, 379, 2264, 5973, 33063],\n",
       "  [40, 765, 284, 2148, 345, 351, 257, 13682, 13]],\n",
       " [[3666, 1438, 318, 1757],\n",
       "  [40, 716, 257, 13682, 33371, 379, 2264, 5973, 33063],\n",
       "  [40, 765, 284, 2148, 345, 351, 257, 13682, 13]]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dataset_personalities(tokenizer, dataset_path='../data/ql_dataset.json', dataset_cache='../data/ql_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalities = get_dataset_personalities(tokenizer, dataset_path='../data/ql_dataset.json', \n",
    "                                          dataset_cache='../data/ql_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "personality = random.choice(personalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.to(args['device'])\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Hi\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-6282a2ed28ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mout_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpersonality\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_history'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-ebecf18faef8>\u001b[0m in \u001b[0;36msample_sequence\u001b[1;34m(personality, history, tokenizer, model, args, current_output)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'no_sample'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'min_length'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mspecial_tokens_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mspecial_tokens_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m                 \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = []\n",
    "while True:\n",
    "    raw_text = input(\">>> \")\n",
    "    while not raw_text:\n",
    "        print('Prompt should not be empty!')\n",
    "        raw_text = input(\">>> \")\n",
    "    if raw_text == 'q':\n",
    "        break\n",
    "        \n",
    "    history.append(tokenizer.encode(raw_text))\n",
    "    with torch.no_grad():\n",
    "        out_ids = sample_sequence(personality, history, tokenizer, model, args)\n",
    "    history.append(out_ids)\n",
    "    history = history[-(2*args['max_history']+1):]\n",
    "    out_text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
    "    print(out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
