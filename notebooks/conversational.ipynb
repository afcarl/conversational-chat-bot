{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from itertools import chain\n",
    "from pytorch_transformers import GPT2DoubleHeadsModel, GPT2Tokenizer, cached_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2DoubleHeadsModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50262, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use 5 special tokens:\n",
    "# - <bos> to indicate the start of the sequence\n",
    "# - <eos> to indicate the end of the sequence\n",
    "# - <speaker1> to indicate the beginning and the tokens of an utterance from the user\n",
    "# - <speaker2> to indicate the beginning and the tokens of an utterance from the bot\n",
    "# - <pad> as a padding token to build batches of sequences\n",
    "special_tokens = {\n",
    "    'bos_token': '<bos>',\n",
    "    'eos_token': '<eos>',\n",
    "    'additional_special_tokens': ['<speaker1>', '<speaker2>'],\n",
    "    'pad_token': '<pad>'\n",
    "}\n",
    "\n",
    "# We can add these special tokens to the vocabulary and the embeddings of the model:\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "#model.config.num_special_tokens = len(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Let's define our contexts and special tokens\n",
    "persona = [[\"i\", \"like\", \"playing\", \"football\", \".\"],\n",
    "           [\"i\", \"am\", \"from\", \"NYC\", \".\"]]\n",
    "history = [[\"hello\", \"how\", \"are\", \"you\", \"?\"],\n",
    "           [\"i\", \"am\", \"fine\", \"thanks\", \".\"]]\n",
    "reply = [\"great\", \"to\", \"hear\"]\n",
    "bos, eos, speaker1, speaker2 = \"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\"\n",
    "\n",
    "def build_inputs(persona, history, reply):\n",
    "    # Build our sequence by adding delimiters and concatenating\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + [eos]]\n",
    "    sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s\n",
    "                                for i, s in enumerate(sequence[1:])]\n",
    "    # Build our word, segments and position inputs from the sequence\n",
    "    words = list(chain(*sequence))                          # word tokens\n",
    "    segments = [speaker2 if i % 2 else speaker1             # segment tokens\n",
    "                for i, s in enumerate(sequence) for _ in s]\n",
    "    position = list(range(len(words)))                      # position tokens\n",
    "    return words, segments, position, sequence\n",
    "\n",
    "words, segments, position, sequence = build_inputs(persona, history, reply)\n",
    "\n",
    "# >>> print(sequence)  # Our inputs looks like this:\n",
    "# [['<bos>', 'i', 'like', 'playing', 'football', '.', 'i', 'am', 'from', 'NYC', '.'],\n",
    "#  ['<speaker1>', 'hello', 'how', 'are', 'you', '?'],\n",
    "#  ['<speaker2>', 'i', 'am', 'fine', 'thanks', '.'],\n",
    "#  ['<speaker1>', 'great', 'to', 'hear', '<eos>']]\n",
    "\n",
    "# Tokenize words and segments embeddings:\n",
    "words = tokenizer.convert_tokens_to_ids(words)\n",
    "segments = tokenizer.convert_tokens_to_ids(segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add a distractor to our previously defined persona, history and reply\n",
    "distractor = [\"sorry\", \"to\", \"hear\", \"that\"]\n",
    "\n",
    "# Build & tokenize inputs ending with our distractor like we did with the gold reply\n",
    "words_distractor, segments_distractor, _, _ = build_inputs(persona, history, distractor)\n",
    "words_distractor = tokenizer.convert_tokens_to_ids(words_distractor)\n",
    "segments_distractor = tokenizer.convert_tokens_to_ids(segments_distractor)\n",
    "\n",
    "# Prepare our language modeling targets: keep only the reply segment, -1 on the rest\n",
    "lm_targets = ([-1] * sum(len(s) for s in sequence[:-1])) \\\n",
    "             + [-1] + tokenizer.convert_tokens_to_ids(sequence[-1][1:])\n",
    "lm_distractor = [-1] * len(words_distractor)\n",
    "\n",
    "# Store the position of the last tokens for the next-sentence prediction loss\n",
    "last_token = len(words) - 1\n",
    "last_token_distractor = len(words_distractor) - 1\n",
    "\n",
    "# Now we can pad reply and distractor inputs and targets to the same length\n",
    "padding_length = max(len(words), len(words_distractor))\n",
    "def pad(x, padding):\n",
    "    return x + [padding] * (padding_length - len(x))\n",
    "\n",
    "(words, words_distractor,\n",
    " segments, segments_distractor) = [pad(x, tokenizer.convert_tokens_to_ids('<pad>'))\n",
    "                                   for x in (words, words_distractor,\n",
    "                                             segments, segments_distractor)]\n",
    "\n",
    "(lm_targets, lm_distractor) = [pad(x, -1) for x in (lm_targets, lm_distractor)]\n",
    " \n",
    "# And gather reply and distractor inputs to build the input tensors:\n",
    "# words tokens\n",
    "input_ids = torch.tensor([[words, words_distractor]], dtype=torch.long)\n",
    "# segment tokens\n",
    "token_type_ids = torch.tensor([[segments, segments_distractor]], dtype=torch.long)\n",
    "# Positions tokens can be automatically created by the model as (0, 1, ..., N)\n",
    "# Last tokens location\n",
    "mc_token_ids = torch.tensor([[last_token, last_token_distractor]], dtype=torch.long)\n",
    "# Language modeling labels\n",
    "lm_labels = torch.tensor([[lm_targets, lm_distractor]], dtype=torch.long)\n",
    "# Next-sentence prediction labels\n",
    "mc_labels = torch.tensor([0], dtype=torch.long)  # Gold reply is 1st (index 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(124.4295, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass\n",
    "lm_loss, mc_loss, _, _, _ = model(input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids)\n",
    "\n",
    "# Total loss as a weighted sum\n",
    "lm_coef = 2.0\n",
    "mc_coef = 1.0\n",
    "total_loss = lm_loss * lm_coef + mc_loss * mc_coef\n",
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/personachat_self_original.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the dataset using our loaded GPT tokenizer\n",
    "def tokenize(obj):\n",
    "    if isinstance(obj, str):\n",
    "        return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "    if isinstance(obj, dict):\n",
    "        return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "    return list(tokenize(o) for o in obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tokenize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-1eab03fdb09a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Get logits with a forward pass in our model (input is pre-defined)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Keep only the last token predictions of the first batch item (batch size 1), apply a temperature coefficient and filter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\altoz\\documents\\projects\\conversational-chat-bot\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\altoz\\documents\\projects\\conversational-chat-bot\\env\\lib\\site-packages\\pytorch_transformers\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids, position_ids, past, head_mask)\u001b[0m\n\u001b[0;32m    710\u001b[0m                 position_ids=None, past=None, head_mask=None):\n\u001b[0;32m    711\u001b[0m         transformer_outputs = self.transformer(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n\u001b[1;32m--> 712\u001b[1;33m                                                past=past, head_mask=head_mask)\n\u001b[0m\u001b[0;32m    713\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\altoz\\documents\\projects\\conversational-chat-bot\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\altoz\\documents\\projects\\conversational-chat-bot\\env\\lib\\site-packages\\pytorch_transformers\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, position_ids, token_type_ids, past, head_mask)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mpast_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mposition_ids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m             \u001b[0mposition_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpast_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpast_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m             \u001b[0mposition_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here is how to use this function for top-p sampling\n",
    "temperature = 1.0\n",
    "top_k = 0\n",
    "top_p = 0.9\n",
    "\n",
    "# Get logits with a forward pass in our model (input is pre-defined)\n",
    "logits = model(input)\n",
    "\n",
    "# Keep only the last token predictions of the first batch item (batch size 1), apply a temperature coefficient and filter\n",
    "logits = logits[0, -1, :] / temperature\n",
    "filtered_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "\n",
    "# Sample from the filtered distribution\n",
    "probabilities = F.softmax(filtered_logits, dim=-1)\n",
    "next_token = torch.multinomial(probabilities, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "from pprint import pformat\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.metrics import Accuracy, Loss, MetricsLambda, RunningAverage\n",
    "from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n",
    "from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler\n",
    "\n",
    "from pytorch_transformers import AdamW, WEIGHTS_NAME, CONFIG_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, dataset_path='../data/personachat_self_original.json', \n",
    "                dataset_cache=cached_path('../data/personachat_self_original.json')):\n",
    "    \"\"\" Get PERSONACHAT from S3 \"\"\"\n",
    "    dataset_path = dataset_path or PERSONACHAT_URL\n",
    "    dataset_cache = dataset_cache + '_' + type(tokenizer).__name__  # Do avoid using GPT cache for GPT-2 and vice-versa\n",
    "    if dataset_cache and os.path.isfile(dataset_cache):\n",
    "        #logger.info(\"Load tokenized dataset from cache at %s\", dataset_cache)\n",
    "        print(\"Load tokenized dataset from cache at %s\", dataset_cache)\n",
    "        dataset = torch.load(dataset_cache)\n",
    "    else:\n",
    "        #logger.info(\"Download dataset from %s\", dataset_path)\n",
    "        print(\"Download dataset from %s\", dataset_path)\n",
    "        personachat_file = cached_path(dataset_path)\n",
    "        with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            dataset = json.loads(f.read())\n",
    "\n",
    "        #logger.info(\"Tokenize and encode the dataset\")\n",
    "        print('Tokenize and encode the dataset')\n",
    "        def tokenize(obj):\n",
    "            if isinstance(obj, str):\n",
    "                return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "            if isinstance(obj, dict):\n",
    "                return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "            return list(tokenize(o) for o in obj)\n",
    "        dataset = tokenize(dataset)\n",
    "        if dataset_cache:\n",
    "            torch.save(dataset, dataset_cache)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_distributed_scalar(scalar, args):\n",
    "    \"\"\" Average a scalar over the nodes if we are in distributed training. We use this for distributed evaluation. \"\"\"\n",
    "    if local_rank == -1:\n",
    "        return scalar\n",
    "    scalar_t = torch.tensor(scalar, dtype=torch.float, device=device) / torch.distributed.get_world_size()\n",
    "    torch.distributed.all_reduce(scalar_t, op=torch.distributed.ReduceOp.SUM)\n",
    "    return scalar_t.item()\n",
    "\n",
    "\n",
    "def pad_dataset(dataset, padding=0):\n",
    "    \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padd only batches but this is simpler. \"\"\"\n",
    "    max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
    "    for name in PADDED_INPUTS:\n",
    "        dataset[name] = [x + [padding if name != \"lm_labels\" else -1] * (max_l - len(x)) for x in dataset[name]]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def build_input_from_segments(persona, history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply \"\"\"\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "\n",
    "    instance = {}\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance[\"lm_labels\"] = [-1] * len(instance[\"input_ids\"])\n",
    "    if lm_labels:\n",
    "        instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][1:]\n",
    "    return instance, sequence\n",
    "\n",
    "\n",
    "def get_data_loaders(tokenizer, dataset_path='../data/personachat_self_original.json', \n",
    "                     dataset_cache=cached_path('../data/personachat_self_original.json')):\n",
    "    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n",
    "    personachat = get_dataset(tokenizer, dataset_path, dataset_cache)\n",
    "\n",
    "    #logger.info(\"Build inputs and labels\")\n",
    "    print('Build inputs and labels')\n",
    "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
    "    for dataset_name, dataset in personachat.items():\n",
    "        num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"])\n",
    "        if num_candidates > 0 and dataset_name == 'train':\n",
    "            num_candidates = min(num_candidates, num_candidates)\n",
    "        for dialog in dataset:\n",
    "            persona = dialog[\"personality\"].copy()\n",
    "            for _ in range(personality_permutations):\n",
    "                for utterance in dialog[\"utterances\"]:\n",
    "                    history = utterance[\"history\"][-(2*max_history+1):]\n",
    "                    for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
    "                        lm_labels = bool(j == num_candidates-1)\n",
    "                        instance, _ = build_input_from_segments(persona, history, candidate, tokenizer, lm_labels)\n",
    "                        for input_name, input_array in instance.items():\n",
    "                            datasets[dataset_name][input_name].append(input_array)\n",
    "                    datasets[dataset_name][\"mc_labels\"].append(num_candidates - 1)\n",
    "                    datasets[dataset_name][\"n_candidates\"] = num_candidates\n",
    "                persona = [persona[-1]] + persona[:-1]  # permuted personalities\n",
    "\n",
    "    #logger.info(\"Pad inputs and convert to Tensor\")\n",
    "    print(\"Pad inputs and convert to Tensor\")\n",
    "    tensor_datasets = {\"train\": [], \"valid\": []}\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
    "        for input_name in MODEL_INPUTS:\n",
    "            tensor = torch.tensor(dataset[input_name])\n",
    "            if input_name != \"mc_labels\":\n",
    "                tensor = tensor.view((-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n",
    "            tensor_datasets[dataset_name].append(tensor)\n",
    "\n",
    "    #logger.info(\"Build train and validation dataloaders\")\n",
    "    print(\"Build train and validation dataloaders\")\n",
    "    train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "    valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset)\n",
    "    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=valid_batch_size, shuffle=False)\n",
    "\n",
    "    #logger.info(\"Train dataset (Batch, Candidates, Seq length): {}\".format(train_dataset.tensors[0].shape))\n",
    "    #logger.info(\"Valid dataset (Batch, Candidates, Seq length): {}\".format(valid_dataset.tensors[0].shape))\n",
    "    \n",
    "    print(\"Train dataset (Batch, Candidates, Seq length): {}\".format(train_dataset.tensors[0].shape))\n",
    "    print(\"Valid dataset (Batch, Candidates, Seq length): {}\".format(valid_dataset.tensors[0].shape))\n",
    "    return train_loader, valid_loader, train_sampler, valid_sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    distributed=False, local_rank=-1, lr = 6.25e-5, dataset_path='../data/personachat_self_original.json', \n",
    "    dataset_cache=cached_path('../data/personachat_self_original.json'),\n",
    "    model_checkpoint='gpt2', num_candidates=2, max_history=2, train_batch_size=4, valid_batch_size=4,\n",
    "    gradient_accumulation_steps=8, lm_coef=1.0, mc_coef=1.0, max_norms=1.0, n_epochs=3, \n",
    "    personality_permutations=1, eval_before_start=False, device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    fp16=''\n",
    "    ):\n",
    "    '''\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--dataset_path\", type=str, default=\"\", help=\"Path or url of the dataset. If empty download from S3.\")\n",
    "    parser.add_argument(\"--dataset_cache\", type=str, default='./dataset_cache', help=\"Path or url of the dataset cache\")\n",
    "    parser.add_argument(\"--model_checkpoint\", type=str, default=\"openai-gpt\", help=\"Path, url or short name of the model\")\n",
    "    parser.add_argument(\"--num_candidates\", type=int, default=2, help=\"Number of candidates for training\")\n",
    "    parser.add_argument(\"--max_history\", type=int, default=2, help=\"Number of previous exchanges to keep in history\")\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=4, help=\"Batch size for training\")\n",
    "    parser.add_argument(\"--valid_batch_size\", type=int, default=4, help=\"Batch size for validation\")\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=8, help=\"Accumulate gradients on several steps\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=6.25e-5, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--lm_coef\", type=float, default=1.0, help=\"LM loss coefficient\")\n",
    "    parser.add_argument(\"--mc_coef\", type=float, default=1.0, help=\"Multiple-choice loss coefficient\")\n",
    "    parser.add_argument(\"--max_norm\", type=float, default=1.0, help=\"Clipping gradient norm\")\n",
    "    parser.add_argument(\"--n_epochs\", type=int, default=3, help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--personality_permutations\", type=int, default=1, help=\"Number of permutations of personality sentences\")\n",
    "    parser.add_argument(\"--eval_before_start\", action='store_true', help=\"If true start with a first evaluation before training\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"Device (cuda or cpu)\")\n",
    "    parser.add_argument(\"--fp16\", type=str, default=\"\", help=\"Set to O0, O1, O2 or O3 for fp16 training (see apex documentation)\")\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"Local rank for distributed training (-1: not distributed)\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # logging is set to INFO (resp. WARN) for main (resp. auxiliary) process. logger.info => log main process only, logger.warning => log all processes\n",
    "    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "    logger.warning(\"Running process %d\", args.local_rank)  # This is a logger.warning: it will be printed by all distributed processes\n",
    "    logger.info(\"Arguments: %s\", pformat(args))\n",
    "    '''\n",
    "    \n",
    "    args = None\n",
    "    \n",
    "    # Initialize distributed training if needed\n",
    "    distributed = (local_rank != -1)\n",
    "    if distributed:\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        device = torch.device(\"cuda\", local_rank)\n",
    "        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "\n",
    "    #logger.info(\"Prepare tokenizer, pretrained model and optimizer - add special tokens for fine-tuning\")\n",
    "    print(\"Prepare tokenizer, pretrained model and optimizer - add special tokens for fine-tuning\")\n",
    "    #tokenizer_class = GPT2Tokenizer if \"gpt2\" in args.model_checkpoint else OpenAIGPTTokenizer\n",
    "    #tokenizer = tokenizer_class.from_pretrained(model_checkpoint)\n",
    "    \n",
    "    #model_class = GPT2DoubleHeadsModel if \"gpt2\" in args.model_checkpoint else OpenAIGPTDoubleHeadsModel\n",
    "    #model = model_class.from_pretrained(args.model_checkpoint)\n",
    "    \n",
    "    #tokenizer.set_special_tokens(SPECIAL_TOKENS)\n",
    "    #model.set_num_special_tokens(len(SPECIAL_TOKENS))\n",
    "    \n",
    "    model = GPT2DoubleHeadsModel.from_pretrained('gpt2')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    # We will use 5 special tokens:\n",
    "    # - <bos> to indicate the start of the sequence\n",
    "    # - <eos> to indicate the end of the sequence\n",
    "    # - <speaker1> to indicate the beginning and the tokens of an utterance from the user\n",
    "    # - <speaker2> to indicate the beginning and the tokens of an utterance from the bot\n",
    "    # - <pad> as a padding token to build batches of sequences\n",
    "    special_tokens = {\n",
    "        'bos_token': '<bos>',\n",
    "        'eos_token': '<eos>',\n",
    "        'additional_special_tokens': ['<speaker1>', '<speaker2>'],\n",
    "        'pad_token': '<pad>'\n",
    "    }\n",
    "\n",
    "    # We can add these special tokens to the vocabulary and the embeddings of the model:\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    #model.config.num_special_tokens = len(special_tokens)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Prepare model for FP16 and distributed training if needed (order is important, distributed should be the last)\n",
    "    if fp16:\n",
    "        from apex import amp  # Apex is only required if we use fp16 training\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=fp16)\n",
    "    if distributed:\n",
    "        model = DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
    "\n",
    "    #logger.info(\"Prepare datasets\")\n",
    "    print('prepare datasets')\n",
    "    train_loader, val_loader, train_sampler, valid_sampler = get_data_loaders(tokenizer)\n",
    "\n",
    "    # Training function and trainer\n",
    "    def update(engine, batch):\n",
    "        model.train()\n",
    "        batch = tuple(input_tensor.to(device) for input_tensor in batch)\n",
    "        lm_loss, mc_loss = model(*batch)\n",
    "        loss = (lm_loss * lm_coef + mc_loss * mc_coef) / gradient_accumulation_steps\n",
    "        if fp16:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_norm)\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        if engine.state.iteration % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "    trainer = Engine(update)\n",
    "\n",
    "    # Evaluation function and evaluator (evaluator output is the input of the metrics)\n",
    "    def inference(engine, batch):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch = tuple(input_tensor.to(device) for input_tensor in batch)\n",
    "            input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "            #logger.info(tokenizer.decode(input_ids[0, -1, :].tolist()))\n",
    "            print(tokenizer.decode(input_ids[0, -1, :].tolist()))\n",
    "            model_outputs = model(input_ids, mc_token_ids, token_type_ids=token_type_ids)\n",
    "            lm_logits, mc_logits = model_outputs[0], model_outputs[1]  # So we can also use GPT2 outputs\n",
    "            lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))\n",
    "            lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)\n",
    "            return (lm_logits_flat_shifted, mc_logits), (lm_labels_flat_shifted, mc_labels)\n",
    "    evaluator = Engine(inference)\n",
    "\n",
    "    # Attach evaluation to trainer: we evaluate when we start the training and at the end of each epoch\n",
    "    trainer.add_event_handler(Events.EPOCH_COMPLETED, lambda _: evaluator.run(val_loader))\n",
    "    if n_epochs < 1:\n",
    "        trainer.add_event_handler(Events.COMPLETED, lambda _: evaluator.run(val_loader))\n",
    "    if eval_before_start:\n",
    "        trainer.add_event_handler(Events.STARTED, lambda _: evaluator.run(val_loader))\n",
    "\n",
    "    # Make sure distributed data samplers split the dataset nicely between the distributed processes\n",
    "    if distributed:\n",
    "        trainer.add_event_handler(Events.EPOCH_STARTED, lambda engine: train_sampler.set_epoch(engine.state.epoch))\n",
    "        evaluator.add_event_handler(Events.EPOCH_STARTED, lambda engine: valid_sampler.set_epoch(engine.state.epoch))\n",
    "\n",
    "    # Linearly decrease the learning rate from lr to zero\n",
    "    scheduler = PiecewiseLinear(optimizer, \"lr\", [(0, lr), (n_epochs * len(train_loader), 0.0)])\n",
    "    trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "\n",
    "    # Prepare metrics - note how we compute distributed metrics \n",
    "    RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n",
    "    metrics = {\"nll\": Loss(torch.nn.CrossEntropyLoss(ignore_index=-1), output_transform=lambda x: (x[0][0], x[1][0])),\n",
    "               \"accuracy\": Accuracy(output_transform=lambda x: (x[0][1], x[1][1]))}\n",
    "    metrics.update({\"average_nll\": MetricsLambda(average_distributed_scalar, metrics[\"nll\"], args),\n",
    "                    \"average_accuracy\": MetricsLambda(average_distributed_scalar, metrics[\"accuracy\"], args)})\n",
    "    metrics[\"average_ppl\"] = MetricsLambda(math.exp, metrics[\"average_nll\"])\n",
    "    for name, metric in metrics.items():\n",
    "        metric.attach(evaluator, name)\n",
    "\n",
    "    # On the main process: add progress bar, tensorboard, checkpoints and save model, configuration and tokenizer before we start to train\n",
    "    if local_rank in [-1, 0]:\n",
    "        pbar = ProgressBar(persist=True)\n",
    "        pbar.attach(trainer, metric_names=[\"loss\"])\n",
    "        evaluator.add_event_handler(Events.COMPLETED, lambda _: pbar.log_message(\"Validation: %s\" % pformat(evaluator.state.metrics)))\n",
    "\n",
    "        tb_logger = TensorboardLogger(log_dir=None)\n",
    "        tb_logger.attach(trainer, log_handler=OutputHandler(tag=\"training\", metric_names=[\"loss\"]), event_name=Events.ITERATION_COMPLETED)\n",
    "        tb_logger.attach(trainer, log_handler=OptimizerParamsHandler(optimizer), event_name=Events.ITERATION_STARTED)\n",
    "        tb_logger.attach(evaluator, log_handler=OutputHandler(tag=\"validation\", metric_names=list(metrics.keys()), another_engine=trainer), event_name=Events.EPOCH_COMPLETED)\n",
    "\n",
    "        checkpoint_handler = ModelCheckpoint(tb_logger.writer.log_dir, 'checkpoint', save_interval=1, n_saved=3)\n",
    "        trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'mymodel': getattr(model, 'module', model)})  # \"getattr\" take care of distributed encapsulation\n",
    "\n",
    "        torch.save(args, tb_logger.writer.log_dir + '/model_training_args.bin')\n",
    "        getattr(model, 'module', model).config.to_json_file(os.path.join(tb_logger.writer.log_dir, CONFIG_NAME))\n",
    "        tokenizer.save_vocabulary(tb_logger.writer.log_dir)\n",
    "\n",
    "    # Run the training\n",
    "    trainer.run(train_loader, max_epochs=n_epochs)\n",
    "\n",
    "    # On the main process: close tensorboard logger and rename the last checkpoint (for easy re-loading with OpenAIGPTModel.from_pretrained method)\n",
    "    if local_rank in [-1, 0] and n_epochs > 0:\n",
    "        os.rename(checkpoint_handler._saved[-1][1][-1], os.path.join(tb_logger.writer.log_dir, WEIGHTS_NAME))  # TODO: PR in ignite to have better access to saved file paths (cleaner)\n",
    "        tb_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare tokenizer, pretrained model and optimizer - add special tokens for fine-tuning\n",
      "prepare datasets\n",
      "Download dataset from %s ../data/personachat_self_original.json\n",
      "Tokenize and encode the dataset\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
